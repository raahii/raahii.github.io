<!doctype html><html lang=ja-jp><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.85.0"><title>Conditional Batch Normalizationについて - 1ミリもわからん</title><link rel=canonical href=https://raahii.github.io/posts/conditional-batch-normalization/><link href=https://raahii.github.io/favicon.ico rel=icon type=image/x-icon><meta property="og:title" content="Conditional Batch Normalizationについて"><meta property="og:description" content="Batch Normalization Batch Normalization(BN)は，内部共変量シフトを軽減することで学習を効率化する手法である．特に学習の初期段階において，前段の層の出力分布が変化すると，後段の層はその変化自体に対応する必要がでてくるため，本質的な非線形関数の学習が阻害されてしまうという問題がある．この問題は層を増やせば増やすほど深刻となる．BNは各層の出力をミニバッチごとに正規化することにより分布の変化を抑制する．また重みの初期値への依存度を下げ，正則化を行う効果もある．
具体的には，入力バッチ $\mathcal{B}= {x_1,\cdot\cdot\cdot,x_m }$ に対して
$$\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=0}^{m} x_i$$
$$\sigma^2_{\mathcal{B}} \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i$$
$$\hat{x}_i \leftarrow \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}}$$
$$y_i \leftarrow \gamma\hat{x}_i + \beta$$
のように，標準化を施し，アフィン変換を行う（新たに平均$\beta$と分散$\gamma^2$を与えるとも言える?）．この$\beta$と$\gamma$がBNの学習パラメータである．また通常，上記の操作は入力特徴マップのチャネルごとに行う．よってパラメータ$\beta$と$\gamma$は長さチャネル数のベクトルとなる．
Conditional Batch Normalization Conditional Batch Normalization1(CBN)の”Conditional”の気持ちはクラスラベルをBNのパラメータ$\gamma$と$\beta$に組み込むところにある．どのように組み込むかというと，下図(右)のように両方のパラメータをクラスラベルを基にMLPでモデル化する（だけ）．
   具体的には，入力データのラベルベクトル$c$があったとき，
$$ \Delta\mathcal{B} = MLP(c),\ \ \ \Delta \gamma = MLP(c) $$
のようにクラスラベルをBNのパラメータのチャネル数に合うようにMLPで変換し，
$$ \hat{\beta} = \beta + \Delta\mathcal{B},\ \ \ \hat{\gamma} = \gamma + \Delta\mathcal{\gamma},$$
のように新たなアフィン変換のパラメータとして用いる．引用したCBNの論文では自然言語のembeddingを用いているが，SNGAN2などではクラスラベルの1-of-Kベクタを用いているはず．
なにが嬉しいのか このあたりが自分もよく把握できていないのが正直なところ．CBN自体は先程触れたSNGANをきっかけに，SAGAN3，BigGAN4でも使われているが，その有無がどれほど精度に影響するのかはあまり言及されていない．おそらく直感的には，従来のような$G$および$D$の最初の層のみにクラスラベルを与えるよりも，様々なレベルの特徴マップに対してクラスラベルを活用するように仕向けることができるのだと思う．
また，各層にクラスラベルを組み込む方法を考えたとき，最もベーシックな方法は1-of-K表現のベクトルを特徴マップのサイズ（FHxFW）に拡大してチャネル方向に結合する手法だが，かなり冗長で，畳み込み演算との相性も微妙と思われる．そういう意味ではCBNを通してクラスラベルを組み込む方が理に適っている可能性はある．
一方，DCGAN5でBNの有効性が示されて以降，GANのgeneratorにBNを用いるのはスタンダードになってきている．そのため，BNをCBNに置き換えたときの計算量の増加，$G$の学習パラメータ増加による学習バランスの悪化が懸念される．
実装 BigGANsのPyTorchによる再現実装にそのコードがあります．標準のBN実装に+αすることでとてもシンプルになっています．"><meta property="og:type" content="article"><meta property="og:url" content="https://raahii.github.io/posts/conditional-batch-normalization/"><meta property="article:section" content="post"><meta property="article:published_time" content="2018-12-12T15:31:51+09:00"><meta property="article:modified_time" content="2018-12-12T15:31:51+09:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Conditional Batch Normalizationについて"><meta name=twitter:description content="Batch Normalization Batch Normalization(BN)は，内部共変量シフトを軽減することで学習を効率化する手法である．特に学習の初期段階において，前段の層の出力分布が変化すると，後段の層はその変化自体に対応する必要がでてくるため，本質的な非線形関数の学習が阻害されてしまうという問題がある．この問題は層を増やせば増やすほど深刻となる．BNは各層の出力をミニバッチごとに正規化することにより分布の変化を抑制する．また重みの初期値への依存度を下げ，正則化を行う効果もある．
具体的には，入力バッチ $\mathcal{B}= {x_1,\cdot\cdot\cdot,x_m }$ に対して
$$\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=0}^{m} x_i$$
$$\sigma^2_{\mathcal{B}} \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i$$
$$\hat{x}_i \leftarrow \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}}$$
$$y_i \leftarrow \gamma\hat{x}_i + \beta$$
のように，標準化を施し，アフィン変換を行う（新たに平均$\beta$と分散$\gamma^2$を与えるとも言える?）．この$\beta$と$\gamma$がBNの学習パラメータである．また通常，上記の操作は入力特徴マップのチャネルごとに行う．よってパラメータ$\beta$と$\gamma$は長さチャネル数のベクトルとなる．
Conditional Batch Normalization Conditional Batch Normalization1(CBN)の”Conditional”の気持ちはクラスラベルをBNのパラメータ$\gamma$と$\beta$に組み込むところにある．どのように組み込むかというと，下図(右)のように両方のパラメータをクラスラベルを基にMLPでモデル化する（だけ）．
   具体的には，入力データのラベルベクトル$c$があったとき，
$$ \Delta\mathcal{B} = MLP(c),\ \ \ \Delta \gamma = MLP(c) $$
のようにクラスラベルをBNのパラメータのチャネル数に合うようにMLPで変換し，
$$ \hat{\beta} = \beta + \Delta\mathcal{B},\ \ \ \hat{\gamma} = \gamma + \Delta\mathcal{\gamma},$$
のように新たなアフィン変換のパラメータとして用いる．引用したCBNの論文では自然言語のembeddingを用いているが，SNGAN2などではクラスラベルの1-of-Kベクタを用いているはず．
なにが嬉しいのか このあたりが自分もよく把握できていないのが正直なところ．CBN自体は先程触れたSNGANをきっかけに，SAGAN3，BigGAN4でも使われているが，その有無がどれほど精度に影響するのかはあまり言及されていない．おそらく直感的には，従来のような$G$および$D$の最初の層のみにクラスラベルを与えるよりも，様々なレベルの特徴マップに対してクラスラベルを活用するように仕向けることができるのだと思う．
また，各層にクラスラベルを組み込む方法を考えたとき，最もベーシックな方法は1-of-K表現のベクトルを特徴マップのサイズ（FHxFW）に拡大してチャネル方向に結合する手法だが，かなり冗長で，畳み込み演算との相性も微妙と思われる．そういう意味ではCBNを通してクラスラベルを組み込む方が理に適っている可能性はある．
一方，DCGAN5でBNの有効性が示されて以降，GANのgeneratorにBNを用いるのはスタンダードになってきている．そのため，BNをCBNに置き換えたときの計算量の増加，$G$の学習パラメータ増加による学習バランスの悪化が懸念される．
実装 BigGANsのPyTorchによる再現実装にそのコードがあります．標準のBN実装に+αすることでとてもシンプルになっています．"><link rel=icon href=https://raahii.github.io/images/favicon.ico type=image/x-icon><link rel=stylesheet href=https://raahii.github.io/css/main.css media=all><link rel=stylesheet href="//fonts.googleapis.com/css?family=Merriweather:400|Lato:400,400italic,700"></head><body><div class=wrapper><header class=header><nav class=nav><a href=https://raahii.github.io/ class=nav-logo><img src=https://raahii.github.io/images/bird.svg width=45 height=45 alt=Logo><p class=nav-title>1ミリもわからん</p></a><ul class=nav-links><li><a href=/tags/>Tags</a></li><li><a href=/about/>About</a></li><li><a href=https://github.com/raahii target=_blank>GitHub</a></li><li><a href=https://twitter.com/raahiiy target=_blank>Twitter</a></li></ul></nav></header><main class=content role=main><article class=article><h1 class=article-title>Conditional Batch Normalizationについて</h1><span class=article-date>December 12, 2018</span>
<span class=article-duration>1 min read</span><ul class="cp_tag01 article-categories">Categories:<li><a href=/categories/%e7%a0%94%e7%a9%b6/>研究</a></li></ul><ul class="cp_tag01 article-tags">Tags:<li><a href=/tags/%e6%a9%9f%e6%a2%b0%e5%ad%a6%e7%bf%92/>機械学習</a></li><li><a href=/tags/%e6%b7%b1%e5%b1%a4%e5%ad%a6%e7%bf%92/>深層学習</a></li><li><a href=/tags/GAN/>GAN</a></li><li><a href=/tags/batch%20normalization/>batch normalization</a></li></ul><div class=article-content><h1 id=batch-normalization>Batch Normalization</h1><p>Batch Normalization(BN)は，内部共変量シフトを軽減することで学習を効率化する手法である．特に学習の初期段階において，前段の層の出力分布が変化すると，後段の層はその変化自体に対応する必要がでてくるため，本質的な非線形関数の学習が阻害されてしまうという問題がある．この問題は層を増やせば増やすほど深刻となる．BNは各層の出力をミニバッチごとに正規化することにより分布の変化を抑制する．また重みの初期値への依存度を下げ，正則化を行う効果もある．</p><p>具体的には，入力バッチ $\mathcal{B}= {x_1,\cdot\cdot\cdot,x_m }$ に対して</p><p>$$\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=0}^{m} x_i$$</p><p>$$\sigma^2_{\mathcal{B}} \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i$$</p><p>$$\hat{x}_i \leftarrow \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}}$$</p><p>$$y_i \leftarrow \gamma\hat{x}_i + \beta$$</p><p>のように，標準化を施し，アフィン変換を行う（新たに平均$\beta$と分散$\gamma^2$を与えるとも言える?）．この$\beta$と$\gamma$がBNの学習パラメータである．また通常，上記の操作は入力特徴マップのチャネルごとに行う．よってパラメータ$\beta$と$\gamma$は長さチャネル数のベクトルとなる．</p><h1 id=conditional-batch-normalization>Conditional Batch Normalization</h1><p>Conditional Batch Normalization<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>(CBN)の”Conditional”の気持ちはクラスラベルをBNのパラメータ$\gamma$と$\beta$に組み込むところにある．どのように組み込むかというと，下図(右)のように両方のパラメータをクラスラベルを基にMLPでモデル化する（だけ）．</p><br><center><figure><img src=/images/2018/conditional-batch-normalization/architecture.png width=600></figure></center><br><p>具体的には，入力データのラベルベクトル$c$があったとき，</p><p>$$ \Delta\mathcal{B} = MLP(c),\ \ \ \Delta \gamma = MLP(c) $$</p><p>のようにクラスラベルをBNのパラメータのチャネル数に合うようにMLPで変換し，</p><p>$$ \hat{\beta} = \beta + \Delta\mathcal{B},\ \ \ \hat{\gamma} = \gamma + \Delta\mathcal{\gamma},$$</p><p>のように新たなアフィン変換のパラメータとして用いる．引用したCBNの論文では自然言語のembeddingを用いているが，SNGAN<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>などではクラスラベルの1-of-Kベクタを用いているはず．</p><h1 id=なにが嬉しいのか>なにが嬉しいのか</h1><p>このあたりが自分もよく把握できていないのが正直なところ．CBN自体は先程触れたSNGANをきっかけに，SAGAN<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>，BigGAN<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>でも使われているが，その有無がどれほど精度に影響するのかはあまり言及されていない．おそらく直感的には，従来のような$G$および$D$の最初の層のみにクラスラベルを与えるよりも，様々なレベルの特徴マップに対してクラスラベルを活用するように仕向けることができるのだと思う．</p><p>また，各層にクラスラベルを組み込む方法を考えたとき，最もベーシックな方法は1-of-K表現のベクトルを特徴マップのサイズ（FHxFW）に拡大してチャネル方向に結合する手法だが，かなり冗長で，畳み込み演算との相性も微妙と思われる．そういう意味ではCBNを通してクラスラベルを組み込む方が理に適っている可能性はある．</p><p>一方，DCGAN<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>でBNの有効性が示されて以降，GANのgeneratorにBNを用いるのはスタンダードになってきている．そのため，BNをCBNに置き換えたときの計算量の増加，$G$の学習パラメータ増加による学習バランスの悪化が懸念される．</p><h1 id=実装>実装</h1><p>BigGANsのPyTorchによる再現実装にそのコードがあります．標準のBN実装に+αすることでとてもシンプルになっています．</p><ul><li><a href=https://github.com/AaronLeong/BigGAN-pytorch/blob/4cbad24f7b49bf55f2b1b6aa8451b2db495b707c/model_resnet.py#L123>BigGAN-pytorch/model_resnet.py at master · AaronLeong/BigGAN-pytorch</a></li></ul><p>またChainerではSNGANの公式実装でコードがあります．CategoricalConditionalBatchNormalizationという名前になっており，このクラスがConditionalBatchNormalizationクラスを継承するという実装になっているのが若干confusingですが…．</p><ul><li><a href=https://github.com/pfnet-research/sngan_projection/blob/e84b1a5f604de5fec268f37c3f26478e80b7f475/source/links/categorical_conditional_batch_normalization.py#L16>sngan_projection/categorical_conditional_batch_normalization.py at master · pfnet-research/sngan_projection</a></li></ul><h1 id=所感>所感</h1><p>実際に研究で使ってみているが，CBN導入で$D$ の学習が早く進みすぎてしまって，モードコラプスになってしまう…．もともと$D$が強めで不安定なのだけど，こういう場合に銀の弾がほぼないのが残念．</p><p>大体の場合，$D$の学習を遅らせるために，</p><ul><li>$D$の学習率を減らす</li><li>PatchGAN導入している場合，$D$の出力の特徴マップのW, Hを大きくする（層を減らす）</li></ul><p>などをして対応するが，かなりケースバイケースなので職人芸感が強くなる…．何か知見が溜まったらまた書きたい．</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Vries, Harm de et al. “Modulating early visual processing by language.” <em>NIPS</em> (2017).&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Miyato, Takeru, and Masanori Koyama. &ldquo;cGANs with projection discriminator.&rdquo; <em>arXiv preprint arXiv:1802.05637</em> (2018).&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>Zhang, Han, et al. &ldquo;Self-Attention Generative Adversarial Networks.&rdquo; <em>arXiv preprint arXiv:1805.08318</em> (2018).&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Brock, Andrew, Jeff Donahue, and Karen Simonyan. &ldquo;Large scale gan training for high fidelity natural image synthesis.&rdquo; <em>arXiv preprint arXiv:1809.11096</em>(2018).&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>Radford, Alec, Luke Metz, and Soumith Chintala. &ldquo;Unsupervised representation learning with deep convolutional generative adversarial networks.&rdquo; <em>arXiv preprint arXiv:1511.06434</em> (2015).&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></article><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//raahii.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a><h3>こちらの記事もどうぞ</h3><ul class="related-content article-content"><li><a href=/posts/dvdgan-adversarial-video-generation-on-complex-datasets/>DVDGAN - "Adversarial Video Generation on Complex Datasets"</a></li><li><a href=/posts/chainer-implementation-3dgan/>3DGANをchainerで実装した</a></li><li><a href=/posts/tool-preview-3d-voxel-data/>ボクセルデータを描画するツールを作った</a></li><li><a href=/posts/nvidia-driver-not-work-after-reboot-on-ubuntu/>Ubuntu16.04でnvidiaドライバが再起動の度に無効になる</a></li><li><a href=/posts/add-video-method-for-tensorboard-chainer/>tensorboard-chainerにビデオを記録するためのPRを出した</a></li></ul></main><footer class=footer><ul class=footer-links><li><a href=https://gohugo.io/ class=footer-links-kudos>Made with <img src=https://raahii.github.io/images/hugo-logo.png width=22 height=22 alt="hugo log"></a></li></ul></footer></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-D4N0KPT2K2"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-D4N0KPT2K2',{anonymize_ip:!1})}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css integrity=sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js integrity=sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></body></html>