<!doctype html><html lang=ja-jp><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.85.0"><title>DVDGAN - "Adversarial Video Generation on Complex Datasets" - 1ミリもわからん</title><link rel=canonical href=https://raahii.github.io/posts/dvdgan-adversarial-video-generation-on-complex-datasets/><link href=https://raahii.github.io/favicon.ico rel=icon type=image/x-icon><meta property="description" content="論文'Adversarial Video Generation on Complex Datasets'のまとめ"><meta property="keywords" content="DVDGAN, video generation, GAN, Generative Adversarial Network, generative model, deep learning, AI"><meta property="og:title" content="DVDGAN - &#34;Adversarial Video Generation on Complex Datasets&#34;"><meta property="og:description" content="論文'Adversarial Video Generation on Complex Datasets'のまとめ"><meta property="og:type" content="article"><meta property="og:url" content="https://raahii.github.io/posts/dvdgan-adversarial-video-generation-on-complex-datasets/"><meta property="og:image" content="https://raahii.github.io/images/2019/dvdgan/arch_legend.png"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-10-29T11:34:06+09:00"><meta property="article:modified_time" content="2019-10-29T11:34:06+09:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://raahii.github.io/images/2019/dvdgan/arch_legend.png"><meta name=twitter:title content="DVDGAN - &#34;Adversarial Video Generation on Complex Datasets&#34;"><meta name=twitter:description content="論文'Adversarial Video Generation on Complex Datasets'のまとめ"><link rel=icon href=https://raahii.github.io/images/favicon.ico type=image/x-icon><link rel=stylesheet href=https://raahii.github.io/css/main.css media=all><link rel=stylesheet href="//fonts.googleapis.com/css?family=Merriweather:400|Lato:400,400italic,700"></head><body><div class=wrapper><header class=header><nav class=nav><a href=https://raahii.github.io/ class=nav-logo><img src=https://raahii.github.io/images/bird.svg width=45 height=45 alt=Logo><p class=nav-title>1ミリもわからん</p></a><ul class=nav-links><li><a href=/tags/>Tags</a></li><li><a href=/about/>About</a></li><li><a href=https://github.com/raahii target=_blank>GitHub</a></li><li><a href=https://twitter.com/raahiiy target=_blank>Twitter</a></li></ul></nav></header><main class=content role=main><article class=article><h1 class=article-title>DVDGAN - "Adversarial Video Generation on Complex Datasets"</h1><span class=article-date>October 29, 2019</span>
<span class=article-duration>3 min read</span><ul class="cp_tag01 article-categories">Categories:<li><a href=/categories/%E7%A0%94%E7%A9%B6/>研究</a></li></ul><ul class="cp_tag01 article-tags">Tags:<li><a href=/tags/%E8%AB%96%E6%96%87/>論文</a></li><li><a href=/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/>機械学習</a></li><li><a href=/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/>深層学習</a></li><li><a href=/tags/%E5%8B%95%E7%94%BB%E7%94%9F%E6%88%90/>動画生成</a></li><li><a href=/tags/gan/>GAN</a></li></ul><div class=article-content><div style="text-align:center;margin:30px auto"><img src=/images/2019/dvdgan/samples.gif alt="generated videos" width=80%></div><p>DeepMindから出た新たな動画生成GANであるDVDGANを読んだのでまとめました．DVDはDual Video Discriminatorの略です📀</p><ul><li><a href=https://arxiv.org/abs/1907.06571>[1907.06571] Adversarial Video Generation on Complex Datasets</a></li><li><a href="https://openreview.net/forum?id=Byx91R4twB&noteId=Byx91R4twB">Adversarial Video Generation on Complex Datasets | OpenReview</a>（ICLR2020投稿中）</li></ul><h2 id=tldr>TL;DR</h2><ul><li>クラスベクトルを用いた条件付き動画生成タスクのGANを提案</li><li>高解像度で長い動画（ $48\times256\times256$ ）の生成に成功</li><li>BigGAN<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>ベースのアーキテクチャを採用</li><li>計算量の削減を目的に，2つの $\mathcal{D}$ を提案:<ul><li>動画中の画像フレームを評価することに特化した$\mathcal{D}_S$</li><li>時間的な変化を評価することに特化した $\mathcal{D}_T$</li></ul></li><li>UCF-101データセットでSOTA</li><li>UCFよりもさらに大きいKinetics-600データセットを使い，様々な動画長・画像サイズでベースラインを提示</li></ul><h2 id=dual-video-discriminator-gan>Dual Video Discriminator GAN</h2><p>従来のGANによる動画生成手法は，モデルアーキテクチャ（主にgenerator）に様々な工夫を行っていました．例えば…</p><ul><li>VideoGAN <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>: 動画は「動的な前景」と「静的な背景」に分けられるという知識を活用．3DCNNで前景に当たる動画を生成し，2DCNNで1枚の背景を生成して合成する．</li></ul><ul><li>FTGAN <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>: 時間的に一貫性を持ったより良い動きを生成するために Optical Flow を活用．VideoGANのアーキテクチャに加えて Optical Flow に条件付けられた動画を生成する．</li><li>MoCoGAN <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>: 動画は時間的に不変である「内容」と，各時刻で異なる「動き」の概念に分けられると主張．1動画を生成する際に「内容」の潜在変数は固定しながら，各フレームごとに異なる「動き」の潜在変数を次々とRNNで生成し，それらを結合した$z$から画像フレームを生成する．</li></ul><p>などがありました．しかしながら，DVDGANではそのような特別な事前知識を用いず，代わりに「高容量のニューラルネットワークをデータドリブンマナーで学習させる」としており，実質，<strong>大量のデータと計算資源で殴る👊</strong>と言っています．</p><p>ではDVDGANの主な貢献は何かというと，BigGAN<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>ベースのアーキテクチャを採用し，$\mathcal{D}$ で計算量を削減することで，より大きな動画の生成ができることを示したことだと思います．全体のモデルアーキテクチャは次のようになっています．</p><div style=text-align:center><figure><img src=/images/2019/dvdgan/arch_legend.png alt="mode architecture" width=100%></figure></div><h3 id=generator>Generator</h3><p>DVDGANでは動画のクラス情報を使った条件付き動画生成を行います．</p><p>まず，正規分布から潜在変数 $z$ をサンプルし，動画のクラスベクトル $y$ のembeddingである $e(y)$ <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>と結合します．どちらも120次元のベクトルです．このベクトルを全結合層を用いて$4\times4$の特徴マップに変換します．</p><p>その後， Convolutional Gated Recurrent Unit (RNN) <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup><sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>→ ResNet Block の処理を繰り返し行い（図青の部分），各時刻の画像フレームを生成します．この処理は1度行うと特徴マップの大きさを2倍にするので，例えば$64\times64$ の画像サイズを持つ動画であれば，4回繰り返すこととなります．肝心の動画の時間変化のモデル化はRNNの状態ベクトル $h$ を時間方向に展開することによって学習しています．この展開は特徴マップが最初にRNNを通過するときに1度だけ行われるとのことです．</p><h3 id=discriminator>Discriminator</h3><p>最初に書いたとおり，2つの $\mathcal{D}$ を持ち，それぞれの役割に基づいて計算量を減らします．</p><p>一つは動画中の画像内容の批判に特化した Spatial Discriminator ($\mathcal{D}_S$)（図上）です．2DのResNet Blockで構成されており， <strong>動画からランダムに抜き出した$k$ 個の画像フレームのみ</strong> を入力します．</p><p>もう一つは時間変化の批判に特化した Temporal Discriminator ($\mathcal{D}_T$)（図下）で，3DのResNet Blockで構成されています．これには動画を入力しますが，<strong>すべての画像フレームにダウンサンプリング処理 $\phi(\cdot)$ を適用して</strong>から入力します．</p><p>時間と空間で担当を分けてしまい，いらない情報は極力削減するということですね．なおそれぞれのハイパラは $k=8$ ， $\phi(\cdot)$ は $2\times2$ のaverge poolingが採用されています．これにより， $\mathcal{D}_S$では $8 \times H \times W$ ，$\mathcal{D}_T$ では$T \times \frac{H}{2} \times \frac{W}{2}$ のピクセルを処理する計算になりますが，これは $48 \times 128 \times 128$の動画を <strong>単純に1つの $\mathcal{D}$ で全ピクセル処理するのと比べると59%の削減</strong>となります．</p><p>ちなみに，$k$ と $\phi(\cdot)$ を決める根拠となった実験も記載されていました． $\phi(\cdot)$ はaverage pooling（$4\times4$，$2\times2$），恒等写像，半分の大きさにランダムクロップするの4通りを，$k$ は1, 4, 8, 10 の4通りを検証しています．</p><div style=text-align:center><figure><img src=/images/2019/dvdgan/fixed_kphi.png alt="k and phi tuning experiment result"></figure></div><p>FIDは低いほうが，ISは高いほうがより良い指標です．$\phi(\cdot)$についてはダイレクトにISを劣化させる結果となり，$k$ は8を超えると効果が薄くなることが見て取れますね．</p><h3 id=training-settings>Training settings</h3><p>GANではモデルに加えてこちらも重要ですが，ほとんどBigGANと同じ…ですかね．ロスには hinge loss <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> を用います．
$$
\mathcal{D}: \min _{\mathcal{D}} \underset{x \sim data(x)}{\mathbb{E}}[\rho(1-\mathcal{D}(x))]+\underset{z \sim p(z)}{\mathbb{E}}[\rho(1+\mathcal{D}(\mathcal{G}(z)))],
$$</p><p>$$
\mathcal{G}: \max _{\mathcal{G}} \underset{z \sim p(z)}{\mathbb{E}}[\mathcal{D}(\mathcal{G}(z))].
$$</p><p>その他のトレーニング設定は次のとおりです．</p><ul><li>すべての重みに Spectral Norm<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>を適用</li><li>初期化は Orhogonal initialization<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup></li><li>バッチサイズ 512</li><li>Adam，lrは$G$が $10^{-4}$，$D$ が $5\cdot10^{-4}$</li><li>$\mathcal{G}$ と $\mathcal{D}$ の更新比は 1:2</li><li>$\mathcal{G}$ では $[z; e(y)]$ を用いて <a href=https://raahii.github.io/2018/12/12/conditional-batch-normalization/>class-conditional Batch Normalization</a> を使用</li><li>$\mathcal{D}$ ではprojection discriminator<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>を使用</li></ul><h2 id=experiment>Experiment</h2><h3 id=ucf-101でsota>UCF-101でSOTA</h3><p>まず，UCF-101データセット<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>で従来手法と IS(Inception Score)<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> で比較をしています．UCF-101は本来は行動認識のためのデータセットですが，動画生成では最もよく用いらます．全101クラス，13320動画で構成されています．</p><p>$16\times128\times128$の動画生成で評価した結果，<strong>DVDGANのISは32.97とTGANv2<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup> の24.34を大きく上回る精度を見せて</strong>います．</p><div style=text-align:center><figure><img src=/images/2019/dvdgan/table2.png alt="inception score for ucf-101 dataset" width=80%></figure></div><p>しかし，appendixで但し書きがされており，DVDGANはそのモデル容量の大きさから部分的に訓練サンプルを記憶している可能性が高いと自ら言っています．その証拠に，サンプルのinterpolation<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>を行うと，滑らかに変化しない場合があったり，訓練サンプルと酷似したサンプルが生成されたようです．（行ごとに独立，動画の最初の1フレームのみ提示）．</p><div style=text-align:center><figure><img src=/images/2019/dvdgan/overfit.png alt="interpolation for ucf-101 dataset" width=100%></figure></div><p>この訓練サンプルの記憶に関し，著者らはInception Scoreという評価指標の欠点であり，同時に，UCF-101は動画生成を検証するのに不十分である（クラス数も各クラス毎のサンプル数も不足している）と指摘しています．</p><h3 id=kinetics-600でベースラインを提示>Kinetics-600でベースラインを提示</h3><p>これを踏まえ，Kinetics-600<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>と呼ばれるUCF101よりもさらに大きなデータセットでも実験を行っています．これもまた行動認識のデータセットで，全600クラス・およそ50万動画と，UCF-101の6倍のクラス数・40倍のサンプル数を擁しています．各クラスには少なくとも600以上のサンプルが収録されているようです．</p><p>このデータセットを使い，論文では今後のベースラインとなるよう様々な解像度・長さの動画でISとFID(Frechet Inception Distance)<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> を提示しています．</p><div style=text-align:center><img src=/images/2019/dvdgan/table1.png alt="score for kinetics-600 dataset" width=80%></div><p>ちなみにWith Truncationの列は，BigGANで提案されたTruncation Trick<sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>と呼ばれるテクニックを用いた場合の最良の値を指しています．</p><p>実験の結果から，小さな動画はきれいなテクスチャや構図，動きを持っているが，動画サイズが大きくなるにつれて，一貫性のある物体や動きの生成が困難になると述べられています．しかしながら，$\mathcal{D}_S$ に入力するフレーム数 $k$ を固定した状態で，生成動画の長さを12→48と増やした時，観測できるフレーム数が相対的には減るにも関わらず，同程度に高解像の動画を学習できることがわかったそうです．やはり問題は動きの学習ということでしょうか．</p><p>また，Kineticsの場合だとinterpolationもうまくいったそうです．</p><div style=text-align:center><figure><img src=/images/2019/dvdgan/intra_interp.png alt="a interpolation for kinetics-600 dataset" width=100%><figcaption>クラスをまたぐ補間</figcaption></figure></div><div style=text-align:center><figure><img src=/images/2019/dvdgan/inter_interp.png alt="a interpolation for kinetics-600 dataset" width=100%><figcaption>同クラスでの補間</figcaption></figure></div><p>実際の生成サンプルは下記リンクに上がっています．</p><ul><li><a href=https://drive.google.com/file/d/155F1lkHA5fMAd7k4W3CQvTsi1eKQDhGb/view>12x64x64</a>, <a href=https://drive.google.com/file/d/165Yxuvvu3viOy-39LhhSDGtczbWphj_i/view>12x128x128</a>, <a href=https://drive.google.com/file/d/1RGRVKCpVaG8z3p9GBCamRk4apiIR7jUc/view>12x256x256</a></li><li><a href=https://drive.google.com/file/d/1FjOQYdUuxPXvS8yeOhXdPQMapUQaklLi/view>48x64x64</a>, <a href=https://drive.google.com/file/d/1P8SsWEGP6tEGPPNPH-iVycOlN6vpIgE8/view>48x128x128</a></li></ul><p>私個人の印象としては，画像はきれいなのですがやはり動きがまだ不自然だと思いました．風景画は良いですが，人や物が絡むと剛体運動をしていない（物自体の形が不自然に変わる）のが気になります．</p><p>ちなみに学習は <strong>32〜512 TPU pods でおよそ12〜96時間</strong> とのことです😰</p><p>実験の章ではさらに動画予測にDVDGANを拡張して実験を行っていますが，これについては割愛します（すいません）．</p><h2 id=所感>所感</h2><ul><li><p>特別な事前知識を使わずにデータで殴るのは正しいと思う．ただ，ここまで大規模なデータでも不自然になるということは，動きをうまく学習できる機構は未だ存在していないと言わざるを得ないと思います．</p></li><li><p>$ \mathcal{D}$ を複数にして，空間と時間で担当を分けるという発想はこれが初めてではない．実際，MoCoGANも全く同様に2つの $\mathcal{D}$ を提案しており（DVDGANの $k=1$ ， $\phi(\cdot)$ が恒等写像である場合に相当する），こうすることで学習の収束が早まると指摘している．そういった意味でも，DVDGANはアーキテクチャの工夫というよりはむしろ，計算量の削減に注力したと言えます．</p></li><li><p>現在の動画生成では計算量と精度に関するトレードオフを評価する慣習はないので，計算量を削減しながらSOTAを更新しているのはすごい．もちろん計算量の削減なしには現実的に難しいこともあるが，単純にスコアを上げるだけならまだ余裕があるように見える．</p></li><li><p>$\mathcal{D}$ の$k$や$\phi$のチューニングに関する実験結果は，FIDは時間的な劣化に疎く，ISは空間的な劣化に疎い，というようにも解釈できる．いずれにしても，ISやFIDが動画の品質を正しく評価できているかは微妙だなぁという実感は日頃からあるので，主観評価もあったらなお良かった気がします．</p></li></ul><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>A. Brock+, 2019, &ldquo;Large Scale GAN Training for High Fidelity Natural Image Synthesis&rdquo;, <a href=https://arxiv.org/abs/1809.11096>https://arxiv.org/abs/1809.11096</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>C. Vondrick+, 2016, &ldquo;Generating Videos with Scene Dynamics&rdquo;, <a href=https://arxiv.org/abs/1609.02612>https://arxiv.org/abs/1609.02612</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>K. Ohnishi+, 2018, &ldquo;Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture&rdquo;, <a href=https://arxiv.org/abs/1711.09618>https://arxiv.org/abs/1711.09618</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>S. Tulyakov+, 2018, &ldquo;MoCoGAN: Decomposing Motion and Content for Video Generation&rdquo;, <a href=https://arxiv.org/abs/1707.04993>https://arxiv.org/abs/1707.04993</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>論文中では &ldquo;a learned linear embedding $e(y)$ of the desired class $y$. &ldquo;と表記されていますが，詳細は<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>にあるのでしょうか…．おそらくですが，クラスを単純な1-of-Kの符号化ではなく，より近い概念であれば近くなるように符号化したい意図があると思います．&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>N. Ballas+, 2015, &ldquo;Delving Deeper into Convolutional Networks for Learning Video Representations&rdquo;, <a href=https://arxiv.org/abs/1511.06432>https://arxiv.org/abs/1511.06432</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>I. Sutskever+, 2011, &ldquo;Generating Text with Recurrent Neural Networks&rdquo;, <a href=https://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf>https://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>J. Hyun+, 2017, &ldquo;Geometric GAN&rdquo;, <a href=https://arxiv.org/abs/1705.02894>https://arxiv.org/abs/1705.02894</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9 role=doc-endnote><p>T. Miyato+, 2018, &ldquo;Spectral Normalization for Generative Adversarial Networks&rdquo;, <a href=https://arxiv.org/abs/1802.05957>https://arxiv.org/abs/1802.05957</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10 role=doc-endnote><p>AM. Saxe+, 2013, &ldquo;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks &ldquo;, <a href=https://arxiv.org/abs/1312.6120>https://arxiv.org/abs/1312.6120</a>&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11 role=doc-endnote><p>T. Miyato, 2018, &ldquo;cGANs with Projection Discriminator&rdquo;, <a href=https://arxiv.org/abs/1802.05637>https://arxiv.org/abs/1802.05637</a>&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12 role=doc-endnote><p>UCF101 - Action Recognition Data Set, <a href=https://www.crcv.ucf.edu/data/UCF101.php>https://www.crcv.ucf.edu/data/UCF101.php</a>&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13 role=doc-endnote><p>T. Salimans+, 2016, &ldquo;Improved Techniques for Training GANs&rdquo;, <a href=https://arxiv.org/abs/1606.03498>https://arxiv.org/abs/1606.03498</a>&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14 role=doc-endnote><p>M. Saito+, 2018, TGANv2: Efficient Training of Large Models for Video Generation with Multiple Subsampling Layers, <a href=https://arxiv.org/abs/1811.09245>https://arxiv.org/abs/1811.09245</a>&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15 role=doc-endnote><p>GAN界隈では，2点の潜在ベクトルの内挿を線形補間などで行い，順にgeneratorで生成してその変化を観測することを指す．&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16 role=doc-endnote><p>Kinetics - DeepMind, <a href=https://deepmind.com/research/open-source/kinetics>https://deepmind.com/research/open-source/kinetics</a>&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17 role=doc-endnote><p>M. Heusel+, 2017, &ldquo;GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium&rdquo;, <a href=https://arxiv.org/abs/1706.08500>https://arxiv.org/abs/1706.08500</a>&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18 role=doc-endnote><p>正規分布の裾の方から取った潜在ベクトルは見た目や構造が崩れたサンプルになりやすいことから，そういった潜在ベクトルを棄却することで，多様性（diversity）を少々犠牲にしつつも本物らしさ（fidelity）を確保しようというテクニック．より正確には<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>を参照．&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></article><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//raahii.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a><h3>こちらの記事もどうぞ</h3><ul class="related-content article-content"><li><a href=/posts/conditional-batch-normalization/>Conditional Batch Normalizationについて</a></li><li><a href=/posts/chainer-implementation-3dgan/>3DGANをchainerで実装した</a></li><li><a href=/posts/tool-preview-3d-voxel-data/>ボクセルデータを描画するツールを作った</a></li><li><a href=/posts/nvidia-driver-not-work-after-reboot-on-ubuntu/>Ubuntu16.04でnvidiaドライバが再起動の度に無効になる</a></li><li><a href=/posts/kinect-rgbd-dataset/>Kinectを用いたRGB-Dデータセットのまとめ</a></li></ul></main><footer class=footer><ul class=footer-links><li><a href=https://gohugo.io/ class=footer-links-kudos>Made with <img src=https://raahii.github.io/images/hugo-logo.png width=22 height=22 alt="hugo log"></a></li></ul></footer></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-D4N0KPT2K2"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-D4N0KPT2K2',{anonymize_ip:!1})}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css integrity=sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js integrity=sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></body></html>