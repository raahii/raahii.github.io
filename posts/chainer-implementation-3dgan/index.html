<!doctype html><html lang=ja-jp><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.85.0"><title>3DGANをchainerで実装した - 1ミリもわからん</title><link rel=canonical href=https://raahii.github.io/posts/chainer-implementation-3dgan/><link href=https://raahii.github.io/favicon.ico rel=icon type=image/x-icon><meta property="og:title" content="3DGANをchainerで実装した"><meta property="og:description" content="タイトルの通り，3DGANのchainer実装をgithubに上げた．当初はKerasで書いていたが良い結果が得られず，ソースコードの間違い探しをするモチベーションが下がってきたので，思い切ってchainerで書き直した．
実はmnistなどのサンプルレベルのものを超えてちゃんとディープラーニングのタスクに取り組むのは今回が初めてだった． ChainerによるGANの実装自体は公式のexampleやchainer-gan-libが非常に参考になった．
モデル 3DGANはその名の通り3Dモデルを生成するためのGAN（Voxelです）．Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modelingで提案されているもの．前回の記事でも触れた．
構造はDCGANと同様で200次元のベクトルよりGeneratorでサンプルを生成，Discriminatorでデータセット由来かGenerator由来か（real/fake）を分類しそのロスをフィードバックする．
Generatorは以下の図（論文より引用）のようなネットワークで，Discriminatorはこれを反転したようなモデルになっている．各ネットワーク内では3次元畳み込みを使用する．最適化手法はAdamで，論文ではDiscriminatorがバッチを8割以上正しく分類できた場合はパラメータを更新しないようにしたとあった．
![f🆔bonhito:20171024233301p:plain](https://cdn-ak.f.st-hatena.com/images/fotolife/b/bonhito/20171024/20171024233301.png &#34;f🆔bonhito:20171024233301p:plain&#34;) 3Dモデル データセットにはShapeNet-v2を用いた．このデータセットには様々な種類の3Dモデルが収録されているが，今回は椅子のモデルのみを抽出した．椅子はおよそ6700サンプルが収録されており，ファイル形式は.binvoxが直接収録されていたのでそれを使用した．
ただ，6700サンプルの3Dデータを全てメモリに乗せることはできなかったため，初期実装では毎回のループで読み込み処理を行っていた．その後，.binvoxファイルのヘッダー読み込みなどが不要であり，処理速度に支障があると感じたので事前に.h5に書き出して使うようにした．
ShapeNet-v2に収録されているデータのサンプルを示す．
実装 3DGANの実装をやろうと決めてから，実験を始める前に3Dモデルの取扱について理解するためのツールをつくっていた．主にはSimple Voxel Viewerで，.binvox形式について理解したり，matplotlibでボクセルをどうやってプロットしようかということについて考えていた．
64x64x64のボクセルを可視化するため，最初はmatplotlibの3Dplotを試したが，scatter plotやsurface plotを使うとマインクラフトのような箱を集積した見映えのプロットが実現できない上，一つ描画するのに数十秒かかることがわかった．そこからまず自作してみようと思いTHREE.jsを使ってSimple Voxel Viewerを作ってみた．ところが結局こっちもいくらか高速化は試したものの，64x64x64のサイズでも密なボクセルになるとメモリーエラーが起こってしまいうまく動作しない問題が起こった．加えて当たり前だがPythonのコードにも組み込めない．
そうして結局，matplotlibの3D volxe plotを採用した．しかしこの関数はまだリリースされていないため（2017/10時点），githubから直接インストールする必要があった．動作も遅いままだが妥協することにした．
ネットワークはKerasやTensorflowなどによる実装がいくつかgithubに上がっていたためそれらも参考にしつつ実装した．加えて，有名なGANのベストプラクティスのページを参考にした．ポイントをかいつまむと以下のような感じで実装した．
 ランダムベクトルzは論文では一様分布だったがガウス分布を使った． GeneratorはDeconv3D+BN+ReLUの繰り返しで，最後だけsigmoid． DiscriminatorはConv3D+BN+Leaky-ReLUの繰り返しで，最後だけsigmoid． Chainerの公式のexampleを真似してロスはsoftplusを使って実装．ただ，実はsigmoid + Adversarial lossがsoftplusと同じなのでDiscriminatorの最後のsigmidは不要なのだが，加えた方がうまくいった(謎)．  結果 成功例 良さげな感じを出すためにきれいなものを集めた．学習の初期段階ではでたらめなものが出力されるが，徐々に椅子が形成され，50エポックから100エポックくらいでましなものが出来た．
学習の途中では椅子とは独立した無意味なかたまりのオブジェクトが所々に浮かんでいたりしたが，それが消えてくるとかなり見栄えが良くなっていった．
失敗例 ボクセルが全て1になったり0になって消滅したりした．今回幾度も学習をさせてみて，初期の段階からほぼ1なボクセル，あるいはほぼ0なボクセルが生成されたり，規則的なパターン（模様）を持つボクセルが生成されたりすると多くの場合失敗となるという微妙な知見を得た．
また，ボクセルが消滅したらその後復活しないこともわかった．ただこれは実装のところで述べたようにロスが間違っているせいかもしれない．
わかったこと  GANはロスは全くあてにならない．生成結果が全て． zはガウス分布から取ってきたほうが良さそう． 学習を調整する(Discriminatorのlossやaccを見て更新しないなど）のはうまくいかないと感じた． 今回のコードではsigmoid + adversarial lossをsoftplusで実装しているので，Discriminatorの最後のsigmoidは不要なはずなのだが，誤って入れていたらうまくいき，外したらうまくいかなくなった．動きゃ勝ちみたいなところがあって釈然としない． 論文では1000エポック学習したとあったが100エポック行かないくらいでかなり形になった．  また，今回はGANのベストプラクティスの内，以下のトリックは実践しても効果がなかった.
 Discriminatorに学習させるミニバッチをrealのみまたはfakeのみにする．(項目4) GeneratorにもDiscriminatorにもLeaky-ReLUをつかう．(項目5) GeneratorにADAMを使ってDiscriminatorにはSGDを使う．(項目10) GeneratorにDropoutを使う．(項目17)  所感 実装に関して，やはりコード自体はKerasの方が圧倒的に簡単にかけるようになっているなと感じた．モデルのインスタンスを作ってバッチをmodel."><meta property="og:type" content="article"><meta property="og:url" content="https://raahii.github.io/posts/chainer-implementation-3dgan/"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-10-25T20:14:00+09:00"><meta property="article:modified_time" content="2017-10-25T20:14:00+09:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="3DGANをchainerで実装した"><meta name=twitter:description content="タイトルの通り，3DGANのchainer実装をgithubに上げた．当初はKerasで書いていたが良い結果が得られず，ソースコードの間違い探しをするモチベーションが下がってきたので，思い切ってchainerで書き直した．
実はmnistなどのサンプルレベルのものを超えてちゃんとディープラーニングのタスクに取り組むのは今回が初めてだった． ChainerによるGANの実装自体は公式のexampleやchainer-gan-libが非常に参考になった．
モデル 3DGANはその名の通り3Dモデルを生成するためのGAN（Voxelです）．Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modelingで提案されているもの．前回の記事でも触れた．
構造はDCGANと同様で200次元のベクトルよりGeneratorでサンプルを生成，Discriminatorでデータセット由来かGenerator由来か（real/fake）を分類しそのロスをフィードバックする．
Generatorは以下の図（論文より引用）のようなネットワークで，Discriminatorはこれを反転したようなモデルになっている．各ネットワーク内では3次元畳み込みを使用する．最適化手法はAdamで，論文ではDiscriminatorがバッチを8割以上正しく分類できた場合はパラメータを更新しないようにしたとあった．
![f🆔bonhito:20171024233301p:plain](https://cdn-ak.f.st-hatena.com/images/fotolife/b/bonhito/20171024/20171024233301.png &#34;f🆔bonhito:20171024233301p:plain&#34;) 3Dモデル データセットにはShapeNet-v2を用いた．このデータセットには様々な種類の3Dモデルが収録されているが，今回は椅子のモデルのみを抽出した．椅子はおよそ6700サンプルが収録されており，ファイル形式は.binvoxが直接収録されていたのでそれを使用した．
ただ，6700サンプルの3Dデータを全てメモリに乗せることはできなかったため，初期実装では毎回のループで読み込み処理を行っていた．その後，.binvoxファイルのヘッダー読み込みなどが不要であり，処理速度に支障があると感じたので事前に.h5に書き出して使うようにした．
ShapeNet-v2に収録されているデータのサンプルを示す．
実装 3DGANの実装をやろうと決めてから，実験を始める前に3Dモデルの取扱について理解するためのツールをつくっていた．主にはSimple Voxel Viewerで，.binvox形式について理解したり，matplotlibでボクセルをどうやってプロットしようかということについて考えていた．
64x64x64のボクセルを可視化するため，最初はmatplotlibの3Dplotを試したが，scatter plotやsurface plotを使うとマインクラフトのような箱を集積した見映えのプロットが実現できない上，一つ描画するのに数十秒かかることがわかった．そこからまず自作してみようと思いTHREE.jsを使ってSimple Voxel Viewerを作ってみた．ところが結局こっちもいくらか高速化は試したものの，64x64x64のサイズでも密なボクセルになるとメモリーエラーが起こってしまいうまく動作しない問題が起こった．加えて当たり前だがPythonのコードにも組み込めない．
そうして結局，matplotlibの3D volxe plotを採用した．しかしこの関数はまだリリースされていないため（2017/10時点），githubから直接インストールする必要があった．動作も遅いままだが妥協することにした．
ネットワークはKerasやTensorflowなどによる実装がいくつかgithubに上がっていたためそれらも参考にしつつ実装した．加えて，有名なGANのベストプラクティスのページを参考にした．ポイントをかいつまむと以下のような感じで実装した．
 ランダムベクトルzは論文では一様分布だったがガウス分布を使った． GeneratorはDeconv3D+BN+ReLUの繰り返しで，最後だけsigmoid． DiscriminatorはConv3D+BN+Leaky-ReLUの繰り返しで，最後だけsigmoid． Chainerの公式のexampleを真似してロスはsoftplusを使って実装．ただ，実はsigmoid + Adversarial lossがsoftplusと同じなのでDiscriminatorの最後のsigmidは不要なのだが，加えた方がうまくいった(謎)．  結果 成功例 良さげな感じを出すためにきれいなものを集めた．学習の初期段階ではでたらめなものが出力されるが，徐々に椅子が形成され，50エポックから100エポックくらいでましなものが出来た．
学習の途中では椅子とは独立した無意味なかたまりのオブジェクトが所々に浮かんでいたりしたが，それが消えてくるとかなり見栄えが良くなっていった．
失敗例 ボクセルが全て1になったり0になって消滅したりした．今回幾度も学習をさせてみて，初期の段階からほぼ1なボクセル，あるいはほぼ0なボクセルが生成されたり，規則的なパターン（模様）を持つボクセルが生成されたりすると多くの場合失敗となるという微妙な知見を得た．
また，ボクセルが消滅したらその後復活しないこともわかった．ただこれは実装のところで述べたようにロスが間違っているせいかもしれない．
わかったこと  GANはロスは全くあてにならない．生成結果が全て． zはガウス分布から取ってきたほうが良さそう． 学習を調整する(Discriminatorのlossやaccを見て更新しないなど）のはうまくいかないと感じた． 今回のコードではsigmoid + adversarial lossをsoftplusで実装しているので，Discriminatorの最後のsigmoidは不要なはずなのだが，誤って入れていたらうまくいき，外したらうまくいかなくなった．動きゃ勝ちみたいなところがあって釈然としない． 論文では1000エポック学習したとあったが100エポック行かないくらいでかなり形になった．  また，今回はGANのベストプラクティスの内，以下のトリックは実践しても効果がなかった.
 Discriminatorに学習させるミニバッチをrealのみまたはfakeのみにする．(項目4) GeneratorにもDiscriminatorにもLeaky-ReLUをつかう．(項目5) GeneratorにADAMを使ってDiscriminatorにはSGDを使う．(項目10) GeneratorにDropoutを使う．(項目17)  所感 実装に関して，やはりコード自体はKerasの方が圧倒的に簡単にかけるようになっているなと感じた．モデルのインスタンスを作ってバッチをmodel."><link rel=icon href=https://raahii.github.io/images/favicon.ico type=image/x-icon><link rel=stylesheet href=https://raahii.github.io/css/main.css media=all><link rel=stylesheet href="//fonts.googleapis.com/css?family=Merriweather:400|Lato:400,400italic,700"></head><body><div class=wrapper><header class=header><nav class=nav><a href=https://raahii.github.io/ class=nav-logo><img src=https://raahii.github.io/images/bird.svg width=45 height=45 alt=Logo><p class=nav-title>1ミリもわからん</p></a><ul class=nav-links><li><a href=/tags/>Tags</a></li><li><a href=/about/>About</a></li><li><a href=https://github.com/raahii target=_blank>GitHub</a></li><li><a href=https://twitter.com/raahiiy target=_blank>Twitter</a></li></ul></nav></header><main class=content role=main><article class=article><h1 class=article-title>3DGANをchainerで実装した</h1><span class=article-date>October 25, 2017</span>
<span class=article-duration>1 min read</span><ul class="cp_tag01 article-categories">Categories:<li><a href=/categories/%e7%a0%94%e7%a9%b6/>研究</a></li></ul><ul class="cp_tag01 article-tags">Tags:<li><a href=/tags/GAN/>GAN</a></li><li><a href=/tags/%e6%b7%b1%e5%b1%a4%e5%ad%a6%e7%bf%92/>深層学習</a></li><li><a href=/tags/3D/>3D</a></li><li><a href=/tags/%e5%ae%9f%e8%a3%85/>実装</a></li><li><a href=/tags/python/>python</a></li></ul><div class=article-content><p>タイトルの通り，3DGANのchainer実装をgithubに上げた．当初はKerasで書いていたが良い結果が得られず，ソースコードの間違い探しをするモチベーションが下がってきたので，思い切ってchainerで書き直した．</p><p><iframe src="https://hatenablog-parts.com/embed?url=https%3A%2F%2Fgithub.com%2Fraahii%2F3dgan-chainer" title=raahii/3dgan-chainer class="embed-card embed-webcard" scrolling=no frameborder=0 style="display:block;width:100%;height:155px;max-width:500px;margin:10px 0"></iframe><cite class=hatena-citation><p>実はmnistなどのサンプルレベルのものを超えてちゃんとディープラーニングのタスクに取り組むのは今回が初めてだった．
ChainerによるGANの実装自体は<a href=https://github.com/chainer/chainer/tree/master/examples/dcgan>公式のexample</a>や<a href=https://github.com/pfnet-research/chainer-gan-lib>chainer-gan-lib</a>が非常に参考になった．</p><h1 id=モデル>モデル</h1><p>3DGANはその名の通り3Dモデルを生成するためのGAN（Voxelです）．<a href=https://arxiv.org/abs/1610.07584>Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling</a>で提案されているもの．<a href=https://raahii.github.io/2017/10/voxel/>前回の記事</a>でも触れた．</p><p>構造はDCGANと同様で200次元のベクトルよりGeneratorでサンプルを生成，Discriminatorでデータセット由来かGenerator由来か（real/fake）を分類しそのロスをフィードバックする．</p><p>Generatorは以下の図（論文より引用）のようなネットワークで，Discriminatorはこれを反転したようなモデルになっている．各ネットワーク内では3次元畳み込みを使用する．最適化手法はAdamで，論文ではDiscriminatorがバッチを8割以上正しく分類できた場合はパラメータを更新しないようにしたとあった．</p><span itemscope itemtype=http://schema.org/Photograph>![f🆔bonhito:20171024233301p:plain](https://cdn-ak.f.st-hatena.com/images/fotolife/b/bonhito/20171024/20171024233301.png "f🆔bonhito:20171024233301p:plain")</span><h1 id=3dモデル>3Dモデル</h1><p>データセットにはShapeNet-v2を用いた．このデータセットには様々な種類の3Dモデルが収録されているが，今回は椅子のモデルのみを抽出した．椅子はおよそ6700サンプルが収録されており，ファイル形式は<code>.binvox</code>が直接収録されていたのでそれを使用した．</p><p>ただ，6700サンプルの3Dデータを全てメモリに乗せることはできなかったため，初期実装では毎回のループで読み込み処理を行っていた．その後，<code>.binvox</code>ファイルのヘッダー読み込みなどが不要であり，処理速度に支障があると感じたので事前に<code>.h5</code>に書き出して使うようにした．</p><p>ShapeNet-v2に収録されているデータのサンプルを示す．</p><img src=https://cdn-ak.f.st-hatena.com/images/fotolife/b/bonhito/20171024/20171024234729.png width=300px>
<img src=https://cdn-ak.f.st-hatena.com/images/fotolife/b/bonhito/20171024/20171024234743.png width=300px>
<img src=https://cdn-ak.f.st-hatena.com/images/fotolife/b/bonhito/20171024/20171024234801.png width=300px>
<img src=https://cdn-ak.f.st-hatena.com/images/fotolife/b/bonhito/20171024/20171024234823.png width=300px><h1 id=実装>実装</h1><p>3DGANの実装をやろうと決めてから，実験を始める前に3Dモデルの取扱について理解するためのツールをつくっていた．主には<a href=https://github.com/raahii/simple_voxel_viewer>Simple Voxel Viewer</a>で，<code>.binvox</code>形式について理解したり，matplotlibでボクセルをどうやってプロットしようかということについて考えていた．</p><p>64x64x64のボクセルを可視化するため，最初はmatplotlibの3Dplotを試したが，<code>scatter plot</code>や<code>surface plot</code>を使うとマインクラフトのような箱を集積した見映えのプロットが実現できない上，一つ描画するのに数十秒かかることがわかった．そこからまず自作してみようと思いTHREE.jsを使って<a href=https://github.com/raahii/simple_voxel_viewer>Simple Voxel Viewer</a>を作ってみた．ところが結局こっちもいくらか高速化は試したものの，64x64x64のサイズでも密なボクセルになるとメモリーエラーが起こってしまいうまく動作しない問題が起こった．加えて当たり前だがPythonのコードにも組み込めない．</p><p>そうして結局，matplotlibの<a href=https://matplotlib.org/devdocs/gallery/mplot3d/voxels.html>3D volxe plot</a>を採用した．しかしこの関数はまだリリースされていないため（2017/10時点），githubから直接インストールする必要があった．動作も遅いままだが妥協することにした．</p><p>ネットワークはKerasやTensorflowなどによる実装がいくつかgithubに上がっていたためそれらも参考にしつつ実装した．加えて，有名な<a href=https://github.com/soumith/ganhacks>GANのベストプラクティス</a>のページを参考にした．ポイントをかいつまむと以下のような感じで実装した．</p><ul><li>ランダムベクトル<code>z</code>は論文では一様分布だったがガウス分布を使った．</li><li>Generatorは<code>Deconv3D</code>+<code>BN</code>+<code>ReLU</code>の繰り返しで，最後だけ<code>sigmoid</code>．</li><li>Discriminatorは<code>Conv3D</code>+<code>BN</code>+<code>Leaky-ReLU</code>の繰り返しで，最後だけ<code>sigmoid</code>．</li><li>Chainerの公式のexampleを真似してロスは<code>softplus</code>を使って実装．ただ，実は<code>sigmoid + Adversarial loss</code>が<code>softplus</code>と同じなので<code>Discriminator</code>の最後の<code>sigmid</code>は不要なのだが，加えた方がうまくいった(謎)．</li></ul><h1 id=結果>結果</h1><h2 id=成功例>成功例</h2><p>良さげな感じを出すためにきれいなものを集めた．学習の初期段階ではでたらめなものが出力されるが，徐々に椅子が形成され，50エポックから100エポックくらいでましなものが出来た．</p><p>学習の途中では椅子とは独立した無意味なかたまりのオブジェクトが所々に浮かんでいたりしたが，それが消えてくるとかなり見栄えが良くなっていった．</p><img src="https://github.com/piyo56/3dgan-chainer/blob/master/result/generated_samples/png/7.png?raw=true" width=300px>
<img src="https://github.com/piyo56/3dgan-chainer/blob/master/result/generated_samples/png/13.png?raw=true" width=300px>
<img src="https://github.com/piyo56/3dgan-chainer/blob/master/result/generated_samples/png/21.png?raw=true" width=300px>
<img src="https://github.com/piyo56/3dgan-chainer/blob/master/result/generated_samples/png/30.png?raw=true" width=300px><h2 id=失敗例>失敗例</h2><p>ボクセルが全て1になったり0になって消滅したりした．今回幾度も学習をさせてみて，初期の段階からほぼ1なボクセル，あるいはほぼ0なボクセルが生成されたり，規則的なパターン（模様）を持つボクセルが生成されたりすると多くの場合失敗となるという微妙な知見を得た．</p><p>また，ボクセルが消滅したらその後復活しないこともわかった．ただこれは実装のところで述べたようにロスが間違っているせいかもしれない．</p><img src=https://i.gyazo.com/1c516e331073f2f35c20948f7e5358b0.png width=300px>
<img src=https://i.gyazo.com/c12285bb038635a17df18410202bc315.png width=300px>
<img src=https://i.gyazo.com/0b87e9115d56faa93332afa61d093ad8.png width=300px>
<img src=https://i.gyazo.com/1e67d6f8872df8b0b6e41d01021f69bf.png width=300px><h1 id=わかったこと>わかったこと</h1><ul><li>GANはロスは全くあてにならない．生成結果が全て．</li><li><code>z</code>は<a href=http://d.hatena.ne.jp/keyword/%A5%AC%A5%A6%A5%B9%CA%AC%C9%DB>ガウス分布</a>から取ってきたほうが良さそう．</li><li>学習を調整する(Discriminatorのlossやaccを見て更新しないなど）のはうまくいかないと感じた．</li><li>今回のコードでは<code>sigmoid + adversarial loss</code>を<code>softplus</code>で実装しているので，<code>Discriminator</code>の最後の<code>sigmoid</code>は不要なはずなのだが，誤って入れていたらうまくいき，外したらうまくいかなくなった．動きゃ勝ちみたいなところがあって釈然としない．</li><li>論文では1000エポック学習したとあったが100エポック行かないくらいでかなり形になった．</li></ul><p>また，今回は<a href=https://github.com/soumith/ganhacks>GANのベストプラクティス</a>の内，以下のトリックは実践しても効果がなかった.</p><ul><li>Discriminatorに学習させるミニバッチをrealのみまたはfakeのみにする．(項目4)</li><li>GeneratorにもDiscriminatorにも<code>Leaky-ReLU</code>をつかう．(項目5)</li><li>Generatorに<code>ADAM</code>を使ってDiscriminatorには<code>SGD</code>を使う．(項目10)</li><li>Generatorに<code>Dropout</code>を使う．(項目17)</li></ul><h1 id=所感>所感</h1><p>実装に関して，やはりコード自体は<code>Keras</code>の方が圧倒的に簡単にかけるようになっているなと感じた．モデルのインスタンスを作ってバッチを<code>model.train_on_batch</code>に渡す処理を繰り返せば良いというイメージでとてもシンプルで良い．DCGANの実装についても，①<code>Descriminator</code>のみのモデル，②<code>Descriminator(not trainable)+Generator</code>のモデルをそれぞれ定義することで学習を実現したが，それぞれ独立に学習，更新処理をやるよというのが表現しやすくてわかりやすかった．</p><p>一方で<code>chainer</code>はまずGPUがある環境とない環境でコードを分けなくてはならず，そこがまず面倒に感じた．自分はリモートマシンが研究室のマシンやGCPなどさまざまなので，手元のmacbookで実装して<code>rsync</code>でデプロイという形で実装していたので処理をいちいち分岐させるのは面倒に感じた．</p><p>加えて<code>trainer</code>, <code>updater</code>辺りの構造を理解するのに学習コストが要るなと感じた．<code>extension</code>もロギングなどよく使う処理をパッケージ化できる所はメリットだと思うが，初めて書く人間にとってはさして複雑でない部分が隠蔽され逆にとっかかりづらいと感じる．慣れてしまえばDCGANの実装もKerasと同じくらいわかりやすくはあったので，忘れないよう継続して使っていきたいと思った．</p><p>あとは学習を繰り返すサイクルでは，ハイパーパラメータのチューニングがべらぼうに難しいという印象を受けた．そしてかなり時間がかかる．今回の3DGANではGCP上の Tesla K80 を使ったが1エポックに15分くらいかかった．1回50エポックで試すとしても一晩寝ても終わってない感じ．生成モデルは辛いですね．</p></div></article><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//raahii.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a><h3>こちらの記事もどうぞ</h3><ul class="related-content article-content"><li><a href=/posts/tool-preview-3d-voxel-data/>ボクセルデータを描画するツールを作った</a></li><li><a href=/posts/dvdgan-adversarial-video-generation-on-complex-datasets/>DVDGAN - "Adversarial Video Generation on Complex Datasets"</a></li><li><a href=/posts/conditional-batch-normalization/>Conditional Batch Normalizationについて</a></li><li><a href=/posts/nvidia-driver-not-work-after-reboot-on-ubuntu/>Ubuntu16.04でnvidiaドライバが再起動の度に無効になる</a></li><li><a href=/posts/files-upload/>画像データをサーバーにPOSTする</a></li></ul></main><footer class=footer><ul class=footer-links><li><a href=https://gohugo.io/ class=footer-links-kudos>Made with <img src=https://raahii.github.io/images/hugo-logo.png width=22 height=22 alt="hugo log"></a></li></ul></footer></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-D4N0KPT2K2"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-D4N0KPT2K2',{anonymize_ip:!1})}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css integrity=sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js integrity=sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></body></html>